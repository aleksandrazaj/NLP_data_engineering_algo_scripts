{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwK5-9FIB-lu"
   },
   "source": [
    "# Bag of words / Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1kiO9kACE6s"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QG7sxmoCIvN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTfaCIzdCLPA"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCK6vQ5QCQJe"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3) #no quote, ignore quotes, 3 anuluje 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qekztq71CixT"
   },
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGUcb5rheCb9",
    "outputId": "34bd0967-8d28-4cc9-c4b7-f3d2ef94fb00"
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "nltk.download('stopwords') #najpierw pobrac, potem importowac\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer #zmiania odmiany na present tense loved-love\n",
    "corpus = [] #zawsze zaczynamy od listy \n",
    "for i in range(0, 1000):\n",
    "  review  = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) #replace anytjing in text, hat meant NOT!!, obok czym zamieniamy - access to JEDNA szczegolna review\n",
    "  review = review.lower() #update review\n",
    "  review = review.split()\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  all_stopwords.remove('not') #usuwa not z tych stowords\n",
    "  all_stopwords.remove(\"isn't\")\n",
    "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)] #get rid off stopwords, for loop in list with if condition (co nie chcemy stemowac), apply steaming do wszystkich slow oprocz stowords\n",
    "  review = ' '.join(review) #dodajesz space\n",
    "  corpus.append(review) #dodajesz review do corpusa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a6lHRZvkugc",
    "outputId": "fdea0c63-ee5b-4c02-8ac7-a1d1ee2d930c"
   },
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLqmAkANCp1-"
   },
   "source": [
    "## Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjEva7qGkefJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500) #instance of a class, pozbadz sie tych slow co nie sa czeste - wtedy usuonie slowa jak steve\n",
    "X = cv.fit_transform(corpus).toarray() #takes all the words puts in columns\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k7_yvgDm9x8",
    "outputId": "b1bd3b88-e0fe-4aad-e6bb-f6cf6ecda87e"
   },
   "outputs": [],
   "source": [
    "len(X[0]) #number of elements in first row of an array - to sa wszystkie slowa najczestesze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH_VjgPzC2cd"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQXYM5VzDDDI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkIq23vEDIPt"
   },
   "source": [
    "## Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DS9oiDXXDRdI",
    "outputId": "f00f54b7-bb05-431d-fc4b-d6eed78a4c23"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JaRM7zXDWUy"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iif0CVhFDaMp",
    "outputId": "db5dd31b-a080-4896-b5da-7174c4d72241"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoMltea5Dir1"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xj9IU6MxDnvo",
    "outputId": "4feb96f2-eeb0-4947-b8a5-9a84878d88c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDOUTKpzpw0L"
   },
   "source": [
    "# Predicting if a single review is positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ9KMsjUp2V9"
   },
   "source": [
    "## Positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a7Xqk-Mp9U2"
   },
   "source": [
    "Use our model to predict if the following review:\n",
    "\n",
    "\"I love this restaurant so much\"\n",
    "\n",
    "is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv4-fNlhp_vG"
   },
   "source": [
    "Solution: We just repeat the same text preprocessing process we did before, but this time with a single review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SENLkBnjps6w",
    "outputId": "085eb4ad-4c9d-4404-817a-6212a67c40d5"
   },
   "outputs": [],
   "source": [
    "new_review = 'I love this restaurant so much'\n",
    "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "new_review = new_review.lower()\n",
    "new_review = new_review.split()\n",
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
    "new_review = ' '.join(new_review)\n",
    "new_corpus = [new_review]\n",
    "new_X_test = cv.transform(new_corpus).toarray()\n",
    "new_y_pred = classifier.predict(new_X_test)\n",
    "print(new_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f18mVLfqMFQ"
   },
   "source": [
    "The review was correctly predicted as positive by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-wRZF1tqMqd"
   },
   "source": [
    "### Negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUpX5rVLqQ1N"
   },
   "source": [
    "Use our model to predict if the following review:\n",
    "\n",
    "\"I hate this restaurant so much\"\n",
    "\n",
    "is positive or negative.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD6P3AXMqWmA"
   },
   "source": [
    "Solution: We just repeat the same text preprocessing process we did before, but this time with a single review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka6wb8VvqY9b",
    "outputId": "7121aa78-e334-470a-80bb-301c6e46386a"
   },
   "outputs": [],
   "source": [
    "new_review = 'I hate this restaurant so much'\n",
    "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
    "new_review = new_review.lower()\n",
    "new_review = new_review.split()\n",
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
    "new_review = ' '.join(new_review)\n",
    "new_corpus = [new_review]\n",
    "new_X_test = cv.transform(new_corpus).toarray()\n",
    "new_y_pred = classifier.predict(new_X_test)\n",
    "print(new_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jj5XBwqIqbq1"
   },
   "source": [
    "The review was correctly predicted as negative by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEeTxx5Tq3y8"
   },
   "source": [
    "2. Evaluate the performance of each of these models. Try to beat the Accuracy obtained in the tutorial. But remember, Accuracy is not enough, so you should also look at other performance metrics like Precision (measuring exactness), Recall (measuring completeness) and the F1 Score (compromise between Precision and Recall). Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score = 2 * Precision * Recall / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_4Y_G-5qqNc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hS-RM8yqeoO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
